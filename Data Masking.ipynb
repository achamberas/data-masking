{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1716a8c9",
   "metadata": {},
   "source": [
    "# Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10011652",
   "metadata": {
    "id": "10011652"
   },
   "outputs": [],
   "source": [
    "pip install openpyxl anonympy xlrd>=1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c4cdc",
   "metadata": {
    "id": "3c6c4cdc"
   },
   "outputs": [],
   "source": [
    "pip install cape-privacy --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5c85bd",
   "metadata": {},
   "source": [
    "# Get File Meta Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779534b1",
   "metadata": {},
   "source": [
    "Enter the path of the data feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2dccc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d83cd",
   "metadata": {},
   "source": [
    "Save a list of the feeds and their attributes to a CSV file and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3eb030",
   "metadata": {
    "id": "ac3eb030"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>object</th>\n",
       "      <th>customer</th>\n",
       "      <th>file</th>\n",
       "      <th>is_gpg</th>\n",
       "      <th>file_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Claims-Medical</td>\n",
       "      <td>ND-BCBS (Medicaid)</td>\n",
       "      <td>BCBSND_PRVGROUP_20230201.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Claims-Medical</td>\n",
       "      <td>ND-BCBS (Medicaid)</td>\n",
       "      <td>BCBSND_MED_CLM_20230201.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Claims-Medical</td>\n",
       "      <td>ND-BCBS (Medicaid)</td>\n",
       "      <td>BCBSND_PRV_20230201.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ADT</td>\n",
       "      <td>Experian</td>\n",
       "      <td>Updated_MemberMatch - Roster Sample File Layou...</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Carepointe</td>\n",
       "      <td>All</td>\n",
       "      <td>revenue_pmpm.csv</td>\n",
       "      <td>False</td>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Carepointe</td>\n",
       "      <td>All</td>\n",
       "      <td>quality-gap-report-by-patient+measure-cut-poin...</td>\n",
       "      <td>False</td>\n",
       "      <td>csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Claims-RX</td>\n",
       "      <td>ND BCBS</td>\n",
       "      <td>BCBSND_RX_CLM_20230301.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Claims-RX</td>\n",
       "      <td>ND NextBlue</td>\n",
       "      <td>NBND_2023_MA_Pharmacy_V7.12_20230220103007.txt</td>\n",
       "      <td>False</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            object            customer  \\\n",
       "5   Claims-Medical  ND-BCBS (Medicaid)   \n",
       "6   Claims-Medical  ND-BCBS (Medicaid)   \n",
       "7   Claims-Medical  ND-BCBS (Medicaid)   \n",
       "19             ADT            Experian   \n",
       "24      Carepointe                 All   \n",
       "30      Carepointe                 All   \n",
       "35       Claims-RX             ND BCBS   \n",
       "46       Claims-RX         ND NextBlue   \n",
       "\n",
       "                                                 file  is_gpg file_type_x  \n",
       "5                        BCBSND_PRVGROUP_20230201.txt   False         txt  \n",
       "6                         BCBSND_MED_CLM_20230201.txt   False         txt  \n",
       "7                             BCBSND_PRV_20230201.txt   False         txt  \n",
       "19  Updated_MemberMatch - Roster Sample File Layou...   False         txt  \n",
       "24                                   revenue_pmpm.csv   False         csv  \n",
       "30  quality-gap-report-by-patient+measure-cut-poin...   False         csv  \n",
       "35                         BCBSND_RX_CLM_20230301.txt   False         txt  \n",
       "46     NBND_2023_MA_Pharmacy_V7.12_20230220103007.txt   False         txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "full_path = []\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if name != '.DS_Store' and '.asc' not in name:\n",
    "            full_path.append(os.path.join(path, name))\n",
    "\n",
    "df = pd.DataFrame(full_path, columns=['full_path'])\n",
    "df = df['full_path'].str.split('/',expand=True)\n",
    "df = df.drop([1,2,3,4], axis=1)\n",
    "df = df.rename(columns={5:'object',6:'customer',7:'file'})\n",
    "df['is_gpg'] = df['file'].str.contains('.gpg')\n",
    "df['temp_file'] = df['file'].replace('.gpg','', regex=True)\n",
    "df['dots'] = df['temp_file'].str.split('.').str.len()\n",
    "df['file_type'] = df.apply(lambda x: x['temp_file'].split('.')[x['dots']-1].lower(), axis=1)\n",
    "df = df.drop(['dots', 'temp_file'], axis=1)\n",
    "\n",
    "df.to_csv('files.csv')\n",
    "\n",
    "df_current = pd.read_excel('data_files.xlsx', sheet_name='files', header=0)\n",
    "\n",
    "df_not_listed = df.merge(df_current.drop_duplicates(), on=['object','customer','file'], \n",
    "                   how='left', indicator=True)\n",
    "\n",
    "df_not_listed[df_not_listed['_merge']=='left_only'][['object','customer','file','is_gpg','file_type_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46379b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# gsheet_name = \"files\"\n",
    "# files = f\"https://docs.google.com/spreadsheets/d/{gsheet_id}/gviz/tq?tqx=out:csv&sheet={gsheet_name}\"\n",
    "# df_files = pd.read_csv(files)\n",
    "# df_files = df_files.rename(columns={'Unnamed: 8':'sheet_id', 'Unnamed: 9':'header_row'})\n",
    "\n",
    "df_files = pd.read_excel('data_files.xlsx', sheet_name='files', header=0)\n",
    "\n",
    "# df_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf00e9",
   "metadata": {},
   "source": [
    "Extract all the fields from each feed and put them into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed9cf1",
   "metadata": {
    "id": "ffed9cf1"
   },
   "outputs": [],
   "source": [
    "# get list of all fields in all files\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_dict = pd.DataFrame()\n",
    "\n",
    "for f in [full_path[29]]:\n",
    "    print(full_path[29])\n",
    "    if '.DS_Store' not in f:\n",
    "        # full_path = base_path + f\n",
    "        f_split = f.split('/')\n",
    "        object = f_split[5]\n",
    "        customer = f_split[6]\n",
    "        file = f_split[7]\n",
    "        dots = len(file.split('.'))\n",
    "        file_type = file.split('.')[dots-1]\n",
    "\n",
    "        dd = pd.DataFrame()\n",
    "        \n",
    "        print(object, customer, file)\n",
    "\n",
    "        if file_type.lower() == 'csv':\n",
    "            df = pd.read_csv(f, nrows=10)\n",
    "            fields = df.dtypes.to_frame('dtypes').reset_index()\n",
    "            dd['field'] = fields['index']\n",
    "            dd['type'] = fields['dtypes']\n",
    "        if file_type.lower() == 'txt' and len(file.split('.')) < 3:\n",
    "            df = pd.read_table(f, delimiter='|', nrows=10, encoding='latin_1')\n",
    "            fields = df.dtypes.to_frame('dtypes').reset_index()\n",
    "            dd['field'] = fields['index']\n",
    "            dd['type'] = fields['dtypes']\n",
    "        if file_type.lower() == 'xlsx':\n",
    "            df = pd.read_excel(f, sheet_name=0, nrows=20, header=1)\n",
    "            fields = df.dtypes.to_frame('dtypes').reset_index()\n",
    "            dd['field'] = fields['index']\n",
    "            dd['type'] = fields['dtypes']\n",
    "        if file_type.lower() == 'xls':\n",
    "            df = pd.read_excel(f, sheet_name=0, nrows=10, engine='xlrd')\n",
    "            fields = df.dtypes.to_frame('dtypes').reset_index()\n",
    "            dd['field'] = fields['index']\n",
    "            dd['type'] = fields['dtypes']\n",
    "            \n",
    "        dd['object'] = object\n",
    "        dd['customer'] = customer\n",
    "        dd['file'] = file\n",
    "        dd['file_type'] = file_type\n",
    "\n",
    "        data_dict = pd.concat([data_dict, dd])\n",
    "\n",
    "data_dict.to_csv('data_files.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1767f2e",
   "metadata": {},
   "source": [
    "# Mask Data\n",
    "\n",
    "Before masking the data, the csv files with feed names and feed columns must be loaded into a Google sheet so that masking rules can be defined.  These rules are based off of the masking functions available in the `anonympy` library.\n",
    "\n",
    "This library will keep all the structure and rows of the feed files, but mask columns specified with masking rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680aeef",
   "metadata": {},
   "source": [
    "Define the id of the Google Sheet and the desired location of the masked data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_id = \"1mrbbbpV3LAEVqewsyY89zFGnfIbohSr2y6zMxqwO4tI\"\n",
    "root = '/Users/anthonychamberas/Desktop/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ed4e2",
   "metadata": {},
   "source": [
    "Functions to create directories if they don't exist and to mask data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9b5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpg_lite as gpg\n",
    "\n",
    "def create_directory(path):\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "def mask_data(df, date_fields, token_fields, fake_fields):\n",
    "    # ensure data types are correct for masked fields\n",
    "    df[date_fields] = df[date_fields].astype('datetime64[ns]')\n",
    "    df[token_fields] = df[token_fields].astype(str)\n",
    "\n",
    "    # mask data in current file per rules\n",
    "    anonym = dfAnonymizer(df)\n",
    "\n",
    "    anonym.categorical_fake(fake_fields)\n",
    "    anonym.datetime_noise(date_fields)\n",
    "    anonym.categorical_tokenization(token_fields, 12, key='bsNhFSaYSU')\n",
    "\n",
    "    dfm = anonym.to_df()\n",
    "    dfm = dfm[df.notna()]\n",
    "    \n",
    "    return dfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa354f8",
   "metadata": {},
   "source": [
    "Loop through each row feed list and apply respective masking rules in the list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "-on3BYgm2Ylp",
   "metadata": {
    "id": "-on3BYgm2Ylp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonychamberas/Library/Python/3.9/lib/python/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start BCBSND-ArkosHealth-Roster-08242023.txt sheet: nan\n",
      "end BCBSND-ArkosHealth-Roster-08242023.txt sheet: nan\n",
      "skipping  NDHIN-ArkosHealth-Roster-headers.txt due to nan\n",
      "skipping  Roster Specifications Flat File Only v1.0 FINA... due to nan\n",
      "start NV-Arkos-Roster-08252023.txt sheet: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/smm2095x7txfhj4s2x6wzqyw0000gn/T/ipykernel_14876/4012688009.py:45: RuntimeWarning: invalid value encountered in cast\n",
      "  sheet_id = sid.astype(int) if df_files.iloc[r]['sheet_id'] else 0\n",
      "/var/folders/xr/smm2095x7txfhj4s2x6wzqyw0000gn/T/ipykernel_14876/4012688009.py:48: RuntimeWarning: invalid value encountered in cast\n",
      "  header_row = hr.astype(int) if hr else 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end NV-Arkos-Roster-08252023.txt sheet: nan\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from anonympy.pandas import dfAnonymizer \n",
    "\n",
    "sheet_name = \"mapping_rules\"\n",
    "map_file = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "df_map_file = pd.read_csv(map_file)\n",
    "\n",
    "sheet_name = \"files\"\n",
    "files = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
    "df_files = pd.read_csv(files)\n",
    "\n",
    "df_files = df_files.rename(columns={'Unnamed: 7':'sheet_id', 'Unnamed: 8':'header_row'})\n",
    "\n",
    "rows = len(df_files.index)\n",
    "start_row = 152\n",
    "rows = 156\n",
    "\n",
    "delimiters = {\n",
    "    \"comma\":\",\",\n",
    "    \"pipe\":\"|\",\n",
    "    \"tab\":\"\\t\"\n",
    "}\n",
    "\n",
    "for r in range(start_row, rows):\n",
    "    \n",
    "    object = df_files.iloc[r]['object']\n",
    "    customer = df_files.iloc[r]['customer']\n",
    "    parent_folder = df_files.iloc[r]['parent_folder']\n",
    "    child_folder = df_files.iloc[r]['child_folder']\n",
    "    file = df_files.iloc[r]['file']\n",
    "    encr = df_files.iloc[r]['encr']\n",
    "    file_type = df_files.iloc[r]['file_type']\n",
    "    delimiter = df_files.iloc[r]['delimiter']\n",
    "    sheet_id = df_files.iloc[r]['sheet_id'].astype(int)\n",
    "    header_row = df_files.iloc[r]['header_row'].astype(int)\n",
    "    skip = df_files.iloc[r]['skip']\n",
    "    notes = df_files.iloc[r]['notes']\n",
    "    \n",
    "    file = file.replace('.gpg', '')\n",
    "    f = 'Input_Files/'+object+'/'+customer+'/'+file\n",
    "    output_path = root + 'Output_Files/'+parent_folder+'/'+child_folder+'/'+file+''\n",
    "    \n",
    "    if skip == 'yes':\n",
    "        print('skipping ',file, 'due to', notes)\n",
    "    else:\n",
    "        print('start',file)\n",
    "\n",
    "        # create sub folders if they don't exist and save results back to a file\n",
    "        path = root + 'Output_Files'\n",
    "        create_directory(path)\n",
    "        path = root + 'Output_Files/'+parent_folder\n",
    "        create_directory(path)\n",
    "        path = root + 'Output_Files/'+parent_folder+'/'+child_folder\n",
    "        create_directory(path)\n",
    "\n",
    "        # get masking rules for current file\n",
    "        dfm = df_map_file[df_map_file['customer'].eq(customer) & df_map_file['object'].eq(object) & df_map_file['file'].eq(file)]\n",
    "\n",
    "        date_fields = dfm[dfm['mask method'].eq('datetime_noise')]['field']\n",
    "\n",
    "        token_fields = dfm[dfm['mask method'].eq('categorical_tokenization')]['field']\n",
    "\n",
    "        fake_fields = dfm[dfm['mask method'].eq('categorical_fake')][['field', 'mask type']]\\\n",
    "            .set_index('field')\\\n",
    "            .to_json(orient='columns')\n",
    "\n",
    "        fake_fields = json.loads(fake_fields)['mask type']\n",
    "\n",
    "        #read data from current file into dataframe\n",
    "\n",
    "        if file_type.lower() == 'csv':\n",
    "            df = pd.read_csv(f)\n",
    "            dfm = mask_data(df, date_fields, token_fields, fake_fields)\n",
    "            dfm.to_csv(root + '/Output_Files/'+mapped_customer+'/'+object+'/'+file+'', index=False)\n",
    "\n",
    "        #### allow for tab delimited and fixed with files ####\n",
    "        if file_type.lower() == 'txt' and delimiter == 'pipe' and len(file.split('.')) < 3:\n",
    "            df = pd.read_table(f, delimiter='|', encoding='latin_1')\n",
    "            dfm = mask_data(df, date_fields, token_fields, fake_fields)\n",
    "            dfm.to_csv(root + '/Output_Files/'+mapped_customer+'/'+object+'/'+file+'', sep='|', index=False)\n",
    "\n",
    "        if file_type.lower() == 'txt' and delimiter == 'tab' and len(file.split('.')) < 3:\n",
    "            df = pd.read_table(f, delimiter='\\t', encoding='latin_1')\n",
    "            dfm = mask_data(df, date_fields, token_fields, fake_fields)\n",
    "            dfm.to_csv(root + '/Output_Files/'+mapped_customer+'/'+object+'/'+file+'', sep='\\t', index=False)\n",
    "\n",
    "        #### mask excel files ####\n",
    "        if file_type.lower() == 'xlsx':\n",
    "            df = pd.read_excel(f, sheet_name=sheet_id, header=header_row)\n",
    "            dfm = mask_data(df, date_fields, token_fields, fake_fields)\n",
    "            dfm.to_excel(root + '/Output_Files/'+mapped_customer+'/'+object+'/'+file+'', index=False)\n",
    "        if file_type.lower() == 'xls':\n",
    "            df = pd.read_excel(f, sheet_name=sheet_id, header=header_row, engine='xlrd')\n",
    "            dfm = mask_data(df, date_fields, token_fields, fake_fields)\n",
    "            dfm.to_excel(root + '/Output_Files/'+mapped_customer+'/'+object+'/'+file+'x', index=False)\n",
    "\n",
    "        print('end',file)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab4648",
   "metadata": {},
   "source": [
    "# Tests and Expirements (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23665895",
   "metadata": {
    "id": "23665895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`numeric`:\n",
      "        * Perturbation - \"numeric_noise\"\n",
      "        * Binning - \"numeric_binning\"\n",
      "        * PCA Masking - \"numeric_masking\"\n",
      "        * Rounding - \"numeric_rounding\"\n",
      "\n",
      "`categorical`:\n",
      "        * Synthetic Data - \"categorical_fake\"\n",
      "        * Synthetic Data Auto - \"categorical_fake_auto\"\n",
      "        * Resampling from same Distribution - \"categorical_resampling\"\n",
      "        * Tokenazation - \"categorical_tokenization\"\n",
      "        * Email Masking - \"categorical_email_masking\"\n",
      "\n",
      "`datetime`:\n",
      "        * Synthetic Date - \"datetime_fake\"\n",
      "        * Perturbation - \"datetime_noise\"\n",
      "\n",
      "`general`:\n",
      "        * Drop Column - \"column_suppression\"\n",
      "        \n",
      "None\n",
      "A | aba, address, administrative_unit, am_pm, android_platform_token, ascii_company_email, ascii_email, ascii_free_email, ascii_safe_email\n",
      "B | bank_country, bban, boolean, bothify, bs, building_number\n",
      "C | cache_pattern, catch_phrase, century, chrome, city, city_prefix, city_suffix, color, color_name, company, company_email, company_suffix, coordinate, country, country_calling_code, country_code, credit_card_expire,credit_card_full, credit_card_number, credit_card_provider, credit_card_security_code, cryptocurrency, cryptocurrency_code, cryptocurrency_name, csv, currency, currency_code, currency_name, currency_symbol, current_country, current_country_code\n",
      "D | date, date_between, date_between_dates, date_object, date_of_birth, date_this_century, date_this_decade, date_this_month, date_this_year, date_time, date_time_ad, date_time_between, date_time_between_dates, date_time_this_century, date_time_this_decade, date_time_this_month, date_time_this_year, day_of_month, day_of_week, del_arguments, dga, domain_name, domain_word, dsv\n",
      "E | ean, ean13, ean8, ein, email\n",
      "F | factories, file_extension, file_name,file_path,firefox, first_name,first_name_female, first_name_male,first_name_nonbinary,fixed_width, format,free_email, free_email_domain, future_date, future_datetime\n",
      "G | generator_attrs, get_arguments, get_formatter, get_providers\n",
      "H | hex_color, hexify, hostname, http_method\n",
      "I | iana_id, iban, image, image_url, internet_explorer, invalid_ssn, ios_platform_token, ipv4, ipv4_network_class, ipv4_private, ipv4_public, ipv6, isbn10, isbn13, iso8601, items, itin\n",
      "J | job, json\n",
      "L | language_code,language_name,last_name,last_name_female, last_name_male, last_name_nonbinary, latitude, latlng, lexify, license_plate, linux_platform_token, linux_processor, local_latlng, locale,locales, localized_ean,localized_ean13, localized_ean8, location_on_land, longitude\n",
      "M | mac_address, mac_platform_token, mac_processor, md5, military_apo, military_dpo, military_ship, military_state, mime_type, month, month_name, msisdn\n",
      "N | name, name_female, name_male, name_nonbinary, nic_handle, nic_handles, null_boolean, numerify\n",
      "O | opera\n",
      "P | paragraph, paragraphs, parse, password, past_date, past_datetime, phone_number, port_number, postalcode, postalcode_in_state, postalcode_plus4, postcode, postcode_in_state, prefix, prefix_female, prefix_male, prefix_nonbinary, pricetag, profile, provider, providers, psv, pybool, pydecimal, pydict, pyfloat, pyint,pyiterable, pylist, pyset, pystr, pystr_format, pystruct, pytimezone, pytuple\n",
      "R | random, random_choices, random_digit, random_digit_not_null, random_digit_not_null_or_empty, random_digit_or_empty, random_element, random_elements, random_int, random_letter, random_letters, random_lowercase_letter, random_number, random_sample, random_uppercase_letter, randomize_nb_elements, rgb_color, rgb_css_color, ripe_id\n",
      "S | safari, safe_color_name, safe_domain_name, safe_email, safe_hex_color, secondary_address, seed_instance, seed_locale, sentence, sentences, set_arguments, set_formatter, sha1, sha256, simple_profile, slug, ssn, state, state_abbr, street_address, street_name, street_suffix, suffix, suffix_female, suffix_male, suffix_nonbinary, swift, swift11, swift8T | tar, text, texts, time, time_delta, time_object, time_series, timezone, tld, tsv\n",
      "U | unique, unix_device, unix_partition, unix_time, upc_a, upc_e, uri, uri_extension, uri_page, uri_path, url, user_agent, user_name, uuid4\n",
      "W | weights, windows_platform_token, word, words\n",
      "Y | year\n",
      "Z | zipcode, zipcode_in_state, zipcode_plus4\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# https://levelup.gitconnected.com/python-data-anonymization-masking-guide-de0b0aa0ca82\n",
    "\n",
    "from anonympy.pandas.utils_pandas import available_methods\n",
    "from anonympy.pandas.utils_pandas import fake_methods\n",
    "\n",
    "print(available_methods())\n",
    "print(fake_methods())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362240e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id sex     masked_id masked_sex\n",
      "0  ABCDEF   M  50b95426ef40          M\n",
      "1  ABCDEF   M  50b95426ef40          M\n",
      "2  UVWXYZ   F  51dfcd54c153          M\n",
      "\n",
      "\n",
      "       id     masked_id\n",
      "0  ABCDEF  50b95426ef40\n",
      "1  ABCDEF  50b95426ef40\n",
      "2  GHIJKL  5dbeec07962e\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from anonympy.pandas import dfAnonymizer \n",
    "\n",
    "df1 = pd.DataFrame([{\"id\":\"ABCDEF\",\"sex\":\"M\"},{\"id\":\"ABCDEF\",\"sex\":\"M\"},{\"id\":\"UVWXYZ\",\"sex\":\"F\"}])\n",
    "df2 = pd.DataFrame([{\"id\":\"ABCDEF\"},{\"id\":\"ABCDEF\"},{\"id\":\"GHIJKL\"}])\n",
    "\n",
    "anonym1 = dfAnonymizer(df1)\n",
    "anonym1.categorical_tokenization('id', 12, key='testkey')\n",
    "#anonym1.categorical_fake({'sex':'random_letter'})\n",
    "anonym1.categorical_resampling('sex')\n",
    "\n",
    "df1['masked_id'] = anonym1.to_df()['id']\n",
    "df1['masked_sex'] = anonym1.to_df()['sex']\n",
    "print(df1)\n",
    "print('\\n')\n",
    "\n",
    "anonym2 = dfAnonymizer(df2)\n",
    "anonym2.categorical_tokenization('id', 12, key='testkey')\n",
    "\n",
    "df2['masked_id'] = anonym2.to_df()['id']\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69002b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_file = '/Users/anthonychamberas/Desktop/Input_Files/Claims-Medical/ND-BCBS (Medicaid)/BCBSND_MED_CLM_20230201.txt.gpg'\n",
    "decrypt(encrypted_file)\n",
    "\n",
    "gpg --decrypt 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_MED_CLM_20230201.txt.gpg' > 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_MED_CLM_20230201.txt.gpg'\n",
    "gpg --decrypt 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_PRV_20230201.txt.gpg' > 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_PRV_20230201.txt'\n",
    "gpg --decrypt 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_PRVGROUP_20230201.txt.gpg' > 'Claims-Medical/ND-BCBS (Medicaid)/BCBSND_PRVGROUP_20230201.txt'\n",
    "gpg --decrypt 'Claims-RX/ND BCBS/BCBSND_RX_CLM_20230301.txt.gpg' > 'Claims-RX/ND BCBS/BCBSND_RX_CLM_20230301.txt'\n",
    "gpg --decrypt 'Claims-RX/ND NextBlue/NBND_2023_MA_Pharmacy_V7.12_20230220103007.txt.gpg' > 'Claims-RX/ND NextBlue/NBND_2023_MA_Pharmacy_V7.12_20230220103007.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpg_lite as gpg\n",
    "\n",
    "gpg_store = gpg.GPGStore()\n",
    "\n",
    "encrypted_file = '/Users/anthonychamberas/Downloads/Input_Files/Claims-Medical/ND-BCBS (Medicaid)/BCBSND_PRVGROUP_20230201.txt.gpg'\n",
    "decrypted_file = encrypted_file[:-4]\n",
    "with open(encrypted_file, \"rb\") as f, open(decrypted_file, \"w\") as f_out:\n",
    "    gpg_store.decrypt(\n",
    "      source=f,\n",
    "      output=f_out,\n",
    "      passphrase=\"Chuck Norris does not need one - the password needs him\")\n",
    "\n",
    "with open(decrypted_file, \"r\") as f:\n",
    "    print(\"Decrypted message:\", \"\".join(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt(encrypted_file):\n",
    "    gpg_store = gpg.GPGStore()\n",
    "    \n",
    "    decrypted_file = encrypted_file[:-4]\n",
    "    with open(encrypted_file, \"rb\") as f, open(decrypted_file, \"w\") as f_out:\n",
    "        gpg_store.decrypt(\n",
    "          source=f,\n",
    "          output=f_out,\n",
    "          passphrase=\"Chuck Norris does not need one - the password needs him\")\n",
    "        \n",
    "    return ('decrypted', encrypted_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d901b8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
